[id="adopting-edpm_{context}"]

//:context: adopting-edpm
//kgilliga: This module might be converted to an assembly, or a procedure as a standalone chapter.
//Check xref contexts.

= Adopting EDPM

== Prerequisites

* Previous Adoption steps completed.
* Remaining source cloud xref:stopping-infrastructure-management-and-compute-services_{context}[Stopping infrastructure management and Compute services] on Compute hosts.

____
*WARNING* This step is a "point of no return" in the EDPM adoption
procedure. The source control plane and data plane services must not
be ever enabled back, after EDPM is deployed, and Podified control
plane has taken control over it.
____

== Variables

Define the shell variables used in the Fast-forward upgrade steps below.
Set `FIP` to the floating IP address of the `test` VM pre-created earlier on the source cloud.
Define the map of compute node name, IP pairs.
The values are just illustrative, use values that are correct for your environment:

----
PODIFIED_DB_ROOT_PASSWORD=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)

alias openstack="oc exec -t openstackclient -- openstack"
FIP=192.168.122.20
declare -A computes
export computes=(
  ["standalone.localdomain"]="192.168.122.100"
  # ...
)
----

== Pre-checks

* Make sure the IPAM is configured

----
oc apply -f - <<EOF
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      - end: 192.168.122.200
        start: 192.168.122.150
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: External
    dnsDomain: external.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 10.0.0.250
        start: 10.0.0.100
      cidr: 10.0.0.0/24
      gateway: 10.0.0.1
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: storagemgmt
    dnsDomain: storagemgmt.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.20.0.250
        start: 172.20.0.100
      cidr: 172.20.0.0/24
      vlan: 23
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
EOF
----

== Procedure - EDPM adoption

* _Temporary fix_ until the OSP 17 https://code.engineering.redhat.com/gerrit/q/topic:stable-compute-uuid[backport of the stable compute UUID feature]
lands.
+
For each compute node grab the UUID of the compute service and write it too
the stable `compute_id` file in `/var/lib/nova/` directory.
+
----
for name in "${!computes[@]}";
do
  uuid=$(\
    openstack hypervisor show $name \
    -f value -c 'id'\
  )
  echo "Writing $uuid to /var/lib/nova/compute_id on $name"
  ssh \
    -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa \
    root@"${computes[$name]}" \
                "echo $uuid | sudo tee /var/lib/nova/compute_id && sudo chown 42436:42436 /var/lib/nova/compute_id && sudo chcon -t container_file_t /var/lib/nova/compute_id"
done
----

* Create a https://kubernetes.io/docs/concepts/configuration/secret/#ssh-authentication-secrets[ssh authentication secret] for the EDPM nodes:
+
----
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: dataplane-adoption-secret
    namespace: openstack
data:
    ssh-privatekey: |
$(cat ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa | base64 | sed 's/^/        /')
EOF
----

* Generate an ssh key-pair `nova-migration-ssh-key` secret
+
----
cd "$(mktemp -d)"
ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  -n openstack \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
rm -f id*
cd -
----

* Create a Nova Compute Extra Config service
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-compute-extraconfig
  namespace: openstack
data:
  19-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: nova-compute-extraconfig
  namespace: openstack
spec:
  label: nova.compute.extraconfig
  configMaps:
    - nova-compute-extraconfig
  secrets:
    - nova-cell1-compute-config
    - nova-migration-ssh-key
  playbook: osp.edpm.nova
EOF
----
+
The secret `nova-cell<X>-compute-config` is auto-generated for each
`cell<X>`. That secret, alongside `nova-migration-ssh-key`, should
always be specified for each custom `OpenStackDataPlaneService` related to Nova.

* Create a repo-setup service to configure Antelope repositories
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: repo-setup
  namespace: openstack
spec:
  label: dataplane.deployment.repo.setup
  play: |
    - hosts: all
      strategy: linear
      tasks:
        - name: Enable podified-repos
          become: true
          ansible.builtin.shell: |
            # TODO: Use subscription-manager and a valid OSP18 repos instead
            # This is a hack to deploy RDO Delorean repos to RHEL as if it were Centos 9 Stream
            set -euxo pipefail
            curl -sL https://github.com/openstack-k8s-operators/repo-setup/archive/refs/heads/main.tar.gz | tar -xz
            python3 -m venv ./venv
            PBR_VERSION=0.0.0 ./venv/bin/pip install ./repo-setup-main
            # This is required for FIPS enabled until trunk.rdoproject.org
            # is not being served from a centos7 host, tracked by
            # https://issues.redhat.com/browse/RHOSZUUL-1517
            dnf -y install crypto-policies
            update-crypto-policies --set FIPS:NO-ENFORCE-EMS
            # FIXME: perform dnf upgrade for other packages in EDPM ansible
            # here we only ensuring that decontainerized libvirt can start
            ./venv/bin/repo-setup current-podified -b antelope -d centos9 --stream
            dnf -y upgrade openstack-selinux
            rm -f /run/virtlogd.pid
            rm -rf repo-setup-main
EOF
----

* Deploy OpenStackDataPlaneNodeSet:
+
Make sure that ovn-controller settings configured in the OpenStackDataPlaneNodeSet are the same as were set in the compute nodes before adoption.
This configuration is stored in the "external_ids" colum in the "Open_vSwitch" table in ovsdb and can be checked with command:
+
----
ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.localdomain, ovn-bridge=br-int, ovn-bridge-mappings="datacentre:br-ctlplane", ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...
----
In above example bridge mappings are set as "datacentre:br-ctlplane" and it has to be set in the OpenStackDataPlaneNodeSet CR also.
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack
spec:
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - repo-setup
    - download-cache
    - bootstrap
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - run-os
    - reboot-os
    - install-certs
    - libvirt
    - nova-compute-extraconfig
    - ovn
    - neutron-metadata
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    standalone:
      hostName: standalone
      ansible:
        ansibleHost: ${computes[standalone.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${computes[standalone.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    managementNetwork: ctlplane
    ansible:
      ansibleUser: root
      ansiblePort: 22
      ansibleVars:
        service_net_map:
          nova_api_network: internalapi
          nova_libvirt_network: internalapi

        # edpm_network_config
        # Default nic config template for a EDPM compute node
        # These vars are edpm_network_config role vars
        edpm_network_config_override: ""
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in role_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in role_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0
        role_networks:
        - InternalApi
        - Storage
        - Tenant
        networks_lower:
          External: external
          InternalApi: internalapi
          Storage: storage
          Tenant: tenant

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: ['datacentre:br-ctlplane']
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_match_northd_version: false
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com

ifeval::["{build}" == "upstream"]
        edpm_ovn_controller_agent_image: quay.io/podified-antelope-centos9/openstack-ovn-controller:current-podified
        edpm_iscsid_image: quay.io/podified-antelope-centos9/openstack-iscsid:current-podified
        edpm_logrotate_crond_image: quay.io/podified-antelope-centos9/openstack-cron:current-podified
        edpm_nova_compute_container_image: quay.io/podified-antelope-centos9/openstack-nova-compute:current-podified
        edpm_nova_libvirt_container_image: quay.io/podified-antelope-centos9/openstack-nova-libvirt:current-podified
        edpm_ovn_metadata_agent_image: quay.io/podified-antelope-centos9/openstack-neutron-metadata-agent-ovn:current-podified
endif::[]
ifeval::["{build}" == "downstream"]
        edpm_ovn_controller_agent_image: registry.redhat.io/rhosp-dev-preview/openstack-ovn-controller-rhel9:18.0
        edpm_iscsid_image: registry.redhat.io/rhosp-dev-preview/openstack-iscsid-rhel9:18.0
        edpm_logrotate_crond_image: registry.redhat.io/rhosp-dev-preview/openstack-cron-rhel9:18.0
        edpm_nova_compute_container_image: registry.redhat.io/rhosp-dev-preview/openstack-nova-compute-rhel9:18.0
        edpm_nova_libvirt_container_image: registry.redhat.io/rhosp-dev-preview/openstack-nova-libvirt-rhel9:18.0
        edpm_ovn_metadata_agent_image: registry.redhat.io/rhosp-dev-preview/openstack-neutron-metadata-agent-ovn-rhel9:18.0
endif::[]

        gather_facts: false
        enable_debug: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']
        # SELinux module
        edpm_selinux_mode: enforcing
        plan: overcloud

        # Do not attempt OVS 3.2 major upgrades here
        edpm_ovs_packages:
        - openvswitch3.1
EOF
----

* Deploy OpenStackDataPlaneDeployment:
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack
spec:
  nodeSets:
  - openstack
EOF
----

* Adoption of the neutron-ovn-metadata-agent:
+
Neutron-ovn-metadata-agent running on the EDPM nodes don't require any
additional actions nor config adjustments to do during the adoption process.
When OpenStackDataPlaneDeployment and OpenStackDataPlaneNodeSet will be ready,
neutron-ovn-metadata-agent should be up and running on the EDPM nodes.

== Post-checks

* Check if all the Ansible EE pods reaches `Completed` status:
+
----
  # watching the pods
  watch oc get pod -l app=openstackansibleee
----
+
----
  # following the ansible logs with:
  oc logs -l app=openstackansibleee -f --max-log-requests 10
----

* Wait for the dataplane node set to reach the Ready status:
+
----
  oc wait --for condition=Ready osdpns/openstack --timeout=30m
----

* Verify that neutron agents are alive:
+
----
oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
----

== Nova compute services fast-forward upgrade from Wallaby to Antelope

Nova services rolling upgrade cannot be done during adoption,
there is in a lock-step with Nova control plane services, because those
are managed independently by EDPM ansible, and Kubernetes operators.
Nova service operator and OpenStack Dataplane operator ensure upgrading
is done independently of each other, by configuring
`[upgrade_levels]compute=auto` for Nova services. Nova control plane
services apply the change right after CR is patched. Nova compute EDPM
services will catch up the same config change with ansible deployment
later on.

____
*NOTE*: Additional orchestration happening around the FFU workarounds
configuration for Nova compute EDPM service is a subject of future changes.
____

* Wait for cell1 Nova compute EDPM services version updated (it may take some time):
+
----
  oc exec openstack-cell1-galera-0 -c galera -- mysql -rs -uroot -p$PODIFIED_DB_ROOT_PASSWORD \
      -e "select a.version from nova_cell1.services a join nova_cell1.services b where a.version!=b.version and a.binary='nova-compute';"
----
+
The above query should return an empty result as a completion criterion.

* Remove pre-FFU workarounds for Nova control plane services:
+
[source,yaml]
----
  oc patch openstackcontrolplane openstack -n openstack --type=merge --patch '
  spec:
    nova:
      template:
        cellTemplates:
          cell0:
            conductorServiceTemplate:
              customServiceConfig: |
                [workarounds]
                disable_compute_service_check_for_ffu=false
          cell1:
            metadataServiceTemplate:
              customServiceConfig: |
                [workarounds]
                disable_compute_service_check_for_ffu=false
            conductorServiceTemplate:
              customServiceConfig: |
                [workarounds]
                disable_compute_service_check_for_ffu=false
        apiServiceTemplate:
          customServiceConfig: |
            [workarounds]
            disable_compute_service_check_for_ffu=false
        metadataServiceTemplate:
          customServiceConfig: |
            [workarounds]
            disable_compute_service_check_for_ffu=false
        schedulerServiceTemplate:
          customServiceConfig: |
            [workarounds]
            disable_compute_service_check_for_ffu=false
  '
----

* Wait for Nova control plane services' CRs to become ready:
+
----
  oc wait --for condition=Ready --timeout=300s Nova/nova
----

* Remove pre-FFU workarounds for Nova compute EDPM services:
+
[source,yaml]
----
  oc apply -f - <<EOF
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: nova-compute-ffu
    namespace: openstack
  data:
    20-nova-compute-cell1-ffu-cleanup.conf: |
      [workarounds]
      disable_compute_service_check_for_ffu=false
  ---
  apiVersion: dataplane.openstack.org/v1beta1
  kind: OpenStackDataPlaneService
  metadata:
    name: nova-compute-ffu
    namespace: openstack
  spec:
    label: nova.compute.ffu
    configMaps:
      - nova-compute-ffu
    secrets:
      - nova-cell1-compute-config
      - nova-migration-ssh-key
    playbook: osp.edpm.nova
  ---
  apiVersion: dataplane.openstack.org/v1beta1
  kind: OpenStackDataPlaneDeployment
  metadata:
    name: openstack-nova-compute-ffu
    namespace: openstack
  spec:
    nodeSets:
      - openstack
    servicesOverride:
      - nova-compute-ffu
  EOF
----

* Wait for Nova compute EDPM service to become ready:
+
----
  oc wait --for condition=Ready osdpd/openstack-nova-compute-ffu --timeout=5m
----

* Run Nova DB online migrations to complete FFU:
+
----
  oc exec -it nova-cell0-conductor-0 -- nova-manage db online_data_migrations
  oc exec -it nova-cell1-conductor-0 -- nova-manage db online_data_migrations
----

* Verify if Nova services can stop the existing test VM instance:
+
----
${BASH_ALIASES[openstack]} server list | grep -qF '| test | ACTIVE |' && openstack server stop test
${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |'
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test | grep "it is in power state shutdown" || echo PASS
----

* Verify if Nova services can start the existing test VM instance:
+
----
${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |' && openstack server start test
${BASH_ALIASES[openstack]} server list | grep -F '| test | ACTIVE |'
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test --fit-width -f json | jq -r '.state' | grep running
----
